Mesos is a cluster resource manager that efficiently shares and isolates resources among multiple distributed applications, or _frameworks_. The current implementation supports "Hadoop"http://hadoop.apache.org, "MPI":http://www.mcs.anl.gov/research/projects/mpich2/, and "Spark":http://github.com/mesos/spark (a new MapReduce-like framework from the Mesos team with support for low-latency interactive use and iterative jobs). Mesos can also serve as a platform upon which to build new cluster computing frameworks, with the powerful advantage that your frameworks will be able to share resources with Hadoop, MPI, etc and can therefore easily be adopted by those communities.

You can use Mesos in several ways:
* To run Hadoop, MPI, Spark and other applications on a shared pool of machines.
* To run multiple instances of Hadoop on the same cluster -- say separate instances for production and experimental jobs -- or even multiple versions of Hadoop (to test out a new version).
* To build new cluster applications without having to reinvent low-level facilities for farming out tasks to different nodes, monitoring them, etc.

This guide explains how to set up and test Mesos, and links to further documentation on other topics.

h1. System Requirements

Mesos currently runs on Linux or Mac OS X, and has previously been tested on OpenSolaris. You will need the following packages to run it:

* g++ 4.1 or higher
* SWIG 1.3.40 or higher
* Python 2.6 (for the Mesos web UI)
* Java 1.6 or higher (to run Java frameworks including Hadoop and "Spark":http://github.com/mesos/spark)

h1. Building Mesos

Mesos uses the standard GNU build tools. You should configure it using the @configure@ script in the root directory. This script accepts the following arguments to enable various options:
* @--with-python-headers=DIR@: Find Python header files in @DIR@ (to turn on Python support). Recommended.
* @--with-webui@: Enable the Mesos web UI (which requires Python 2.6). Recommended.
* @--with-java-home=DIR@: Enable Java framework support with a given installation of Java. Required for Hadoop and Spark.
* @--with-zookeeper=DIR@ or @--with-included-zookeeper@: Enable master fault-tolerance using either an existing ZooKeeper installation or the version of ZooKeeper bundled with Mesos. For more help, see [[using ZooKeeper]].

The Git repository contains sample configure scripts for Mac OS X and several recent Linux distributions, with the correct paths for each platform. These still assume that you have installed the correct packages (e.g. @python-dev@ and the JDK on Linux).

h1. Testing the build

After you build Mesos, you can run its unit tests using the @alltests@ program located in @src@. Note that a few tests for specific platforms are disabled by default. You can run @alltests@ with @--help@ for help about its options.

You can also set up a small Mesos cluster as follows:

# Go into the @src@ directory of your installation.
# Type @./mesos-master@ to launch the master.
# Take note of the master identifier that is printed to stdout, which will look like @mesos://1@192.168.0.1:5050@.
# View the master's web UI at @http://[hostname of master]:8080@.
# Launch a slave by typing @./mesos-slave --url=[URL of master]@. The slave will show up on the web UI if you refresh it.
# Run the C++ test framework (a sample application which just executes five tasks on the cluster) using @./cpp-test-framework [URL of master]@. It should successfully finish after running five tasks.

h1. Logging information

Mesos uses "Google Log":http://code.google.com/p/google-glog and writes logs to @MESOS_HOME/logs@ by default, where @MESOS_HOME@ is the location where Mesos is installed. The log directory can be [[configured|Configuration]] using the @log_dir@ parameter.

Frameworks that run on Mesos have their output stored to a "work" directory on each machine. By default, this is @MESOS_HOME/work@. Within this directory, a framework's output is placed in files called @stdout@ and @stderr@ in a directory of the form @slave-X/fw-Y/Z@, where X is the slave ID, Y is the framework ID, and multiple subdirectories Z are created for each attempt to run an executor for the framework. These files can also be accessed via the web UI of the slave daemon (which is linked from the master web UI).

Finally, structured logs of cluster activity can be written to a tab-separated text file or a SQLite database using the [[event history]] system.

h1. Developing using "local" 

To ease framework development and testing, instead of running a master plus a slave daemon and then hunting down @stdout@, @stderr@, etc, you can pass @local@ as the master URL argument to your framework scheduler, which will run a single master and slave in the same process and write all output to the standard output and error streams. By default, Mesos logs will also be written to these streams. You can disable these logs by using @localquiet@ as the URL.

h1. Where to go from here

* [[Framework development guide]]
* [[Mesos configuration|Configuration]]
* [[Running Hadoop on Mesos]]
* [[Running MPI on Mesos]]
* [[Running Spark on Mesos]]
* [[Mesos internal architecture]]
* [[Mesos development roadmap]]